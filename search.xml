<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>HTTP2的问题</title>
      <link href="/2024/05/02/HTTP2%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2024/05/02/HTTP2%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="HTTP-2"><a href="#HTTP-2" class="headerlink" title="HTTP&#x2F;2"></a>HTTP&#x2F;2</h1><h2 id="HTTP-2-的问题"><a href="#HTTP-2-的问题" class="headerlink" title="HTTP&#x2F;2 的问题"></a><strong>HTTP&#x2F;2 的问题</strong></h2><p><strong>队头阻塞</strong></p><p><strong>HTTP&#x2F;2解决了HTTP的队头阻塞问题，但是并没有解决TCP队头阻塞问题！</strong></p><p>HTTP&#x2F;1.1相比较于HTTP&#x2F;1.0来说，最主要的改进就是引入了持久连接（keep-alive）。</p><p><strong>所谓的持久连接就是：在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。</strong></p><p>引入了持久连接之后，在性能方面，HTTP协议有了明显的提升。</p><p>HTTP&#x2F;1.1允许在持久连接上使用请求管道，是相对于持久连接的又一性能优化。</p><p>所谓请求管道，就是在HTTP响应到达之前，可以将多条请求放入队列，当第一条HTTP请求通过网络流向服务器时，第二条和第三条请求也可以开始发送了。在高时延网络条件下，这样做可以降低网络的环回时间，提高性能。</p><p><strong>但是，对于管道连接还是有一定的限制和要求的，其中一个比较关键的就是服务端必须按照与请求相同的顺序回送HTTP响应。</strong></p><p>这也就意味着，如果一个响应返回发生了延迟，那么其后续的响应都会被延迟，直到队头的响应送达。这就是所谓的<strong>HTTP队头阻塞</strong>。</p><p>但是HTTP队头阻塞的问题在HTTP&#x2F;2中得到了有效的解决。<strong>HTTP&#x2F;2废弃了管道化的方式</strong>，而是创新性的引入了帧、消息和数据流等概念。<strong>客户端和服务器可以把 HTTP 消息分解为互不依赖的帧，然后乱序发送，最后再在另一端把它们重新组合起来。</strong></p><p><strong>因为没有顺序了，所以就不需要阻塞了，就有效的解决了HTTP对头阻塞的问题。</strong></p><p>但是，HTTP&#x2F;2仍然会存在对头阻塞的问题，那是因为HTTP&#x2F;2其实还是依赖TCP协议实现的。</p><p>TCP传输过程中会把数据拆分为一个个<strong>按照顺序</strong>排列的数据包，这些数据包通过网络传输到了接收端，接收端再<strong>按照顺序</strong>将这些数据包组合成原始数据，这样就完成了数据传输。</p><p>但是如果其中的某一个数据包没有按照顺序到达，接收端会一直保持连接等待数据包返回，这时候就会阻塞后续请求。这就发生了<strong>TCP队头阻塞</strong>。</p><p>HTTP&#x2F;1.1的管道化持久连接也是使得同一个TCP链接可以被多个HTTP使用，但是HTTP&#x2F;1.1中规定一个域名可以有6个TCP连接。而HTTP&#x2F;2中，同一个域名只是用一个TCP连接。</p><p>所以，<strong>在HTTP&#x2F;2中，TCP对头阻塞造成的影响会更大</strong>，因为HTTP&#x2F;2的多路复用技术使得多个请求其实是基于同一个TCP连接的，那如果某一个请求造成了TCP队头阻塞，那么多个请求都会受到影响。</p><h2 id="升级TCP是否可行？"><a href="#升级TCP是否可行？" class="headerlink" title="升级TCP是否可行？"></a><strong>升级TCP是否可行？</strong></h2><p>基于上面我们提到的这些问题，很多人提出来说：既然TCP存在这些问题，并且我们也知道这些问题的存在，甚至解决方案也不难想到，为什么不能对协议本身做一次升级，解决这些问题呢？</p><p>这就涉及到一个”<strong>协议僵化</strong>“的问题。</p><p>我们知道的，想要在家里使用网络有几个前提，首先我们要通过运行商开通网络，并且需要使用路由器，而路由器就是网络传输过程中的一个中间设备。</p><blockquote><p>中间设备是指插入在数据终端和信号转换设备之间，完成调制前或解调后某些附加功能的辅助设备。例如集线器、交换机和无线接入点、路由器、安全解调器、通信服务器等都是中间设备。</p></blockquote><p>在我们看不到的地方，这种中间设备还有很多很多，<strong>一个网络需要经过无数个中间设备的转发才能到达终端用户。</strong></p><p>如果TCP协议需要升级，那么意味着需要这些中间设备都能支持新的特性，我们知道路由器我们可以重新换一个，但是其他的那些中间设备呢？尤其是那些比较大型的设备呢？更换起来的成本是巨大的。</p><p>而且，除了中间设备之外，操作系统也是一个重要的因素，<strong>因为TCP协议需要通过操作系统内核来实现，而操作系统的更新也是非常滞后的。</strong></p><p>所以，这种问题就被称之为”中间设备僵化”，也是导致”协议僵化”的重要原因。这也是限制着TCP协议更新的一个重要原因。</p><p>所以，近些年来，由IETF标准化的许多TCP新特性都因缺乏广泛支持而没有得到广泛的部署或使用！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>因为HTTP&#x2F;2底层是采用TCP协议实现的，虽然解决了HTTP队头阻塞的问题，但是对于TCP队头阻塞的问题却无能为力。</p><p>TCP传输过程中会把数据拆分为一个个<strong>按照顺序</strong>排列的数据包，这些数据包通过网络传输到了接收端，接收端再<strong>按照顺序</strong>将这些数据包组合成原始数据，这样就完成了数据传输。</p><p>但是如果其中的某一个数据包没有按照顺序到达，接收端会一直保持连接等待数据包返回，这时候就会阻塞后续请求。这就发生了<strong>TCP队头阻塞</strong>。</p><p>另外，TCP这种可靠传输是靠三次握手实现的，TCP三次握手的过程客户端和服务器之间需要交互三次，那么也就是说需要消耗1.5 RTT。如果是HTTPS那么消耗的RTT就更多。</p><p>而因为很多中间设备比较陈旧，更新换代成本巨大，这就导致TCP协议升级或者采用新的协议基本无法实现。</p><p>所以，HTTP&#x2F;3选择了一种新的技术方案，那就是基于UDP做改造，这种技术叫做QUIC。</p><h1 id="HTTP3"><a href="#HTTP3" class="headerlink" title="HTTP3"></a>HTTP3</h1><h2 id="QUIC协议"><a href="#QUIC协议" class="headerlink" title="QUIC协议"></a><strong>QUIC协议</strong></h2><p>HTTP&#x2F;2之所以”被弃用”，是因为他使用的传输层协议仍然是TCP，所以HTTP&#x2F;3首要解决的问题就是绕开TCP。</p><p>那么如果研发一种新的协议，同样还是会因为受到中间设备僵化的影响，导致无法被大规模应用。所以，研发人员们想到了一种基于UDP实现的方式。</p><p>于是，Google是最先采用这种方式并付诸于实践的，他们在2013年推出了一种叫做QUIC的协议，全称是Quick UDP Internet Connections。</p><p>在设计之初，Google就希望使用这个协议来取代HTTPS&#x2F;HTTP协议，使网页传输速度加快。2015年6月，QUIC的网络草案被正式提交至互联网工程任务组。2018 年 10 月，互联网工程任务组 HTTP 及 QUIC 工作小组正式将基于 QUIC 协议的 HTTP（英语：HTTP over QUIC）重命名为HTTP&#x2F;3。</p><p><strong>所以，我们现在所提到的HTTP&#x2F;3，其实就是HTTP over QUIC，即基于QUIC协议实现的HTTP。</strong></p><p>QUIC协议有以下特点：</p><ul><li>基于UDP的传输层协议：它使用UDP端口号来识别指定机器上的特定服务器。</li><li>可靠性：虽然UDP是不可靠传输协议，但是QUIC在UDP的基础上做了些改造，使得<strong>他提供了和TCP类似的可靠性</strong>。它提供了数据包重传、拥塞控制、调整传输节奏以及其他一些TCP中存在的特性。</li><li>实现了无序、并发字节流：<strong>QUIC的单个数据流可以保证有序交付，但多个数据流之间可能乱序</strong>，这意味着单个数据流的传输是按序的，但是多个数据流中接收方收到的顺序可能与发送方的发送顺序不同！</li><li>快速握手：<strong>QUIC提供0-RTT和1-RTT的连接建立</strong></li><li>使用TLS 1.3传输层安全协议：与更早的TLS版本相比，TLS 1.3有着很多优点，但使用它的最主要原因是其握手所花费的往返次数更低，从而能降低协议的延迟。</li></ul><p>QUIC到底属于TCP&#x2F;IP协议族中的那一层呢？我们知道，QUIC是基于UDP实现的，并且是HTTP&#x2F;3的所依赖的协议，那么，按照TCP&#x2F;IP的分层来讲，他是属于传输层的，也就是和TCP、UDP属于同一层。</p><p>更加细化一点的话，因为QUIC不仅仅承担了传输层协议的职责，还具备了TLS的安全性相关能力，所以，可以通过下图来理解QUIC在HTTP&#x2F;3的实现中所处的位置。</p><h2 id="QUIC的连接建立"><a href="#QUIC的连接建立" class="headerlink" title="QUIC的连接建立"></a><strong>QUIC的连接建立</strong></h2><p>QUIC提出一种新的连接建立机制，基于这种连接接机制，实现了快速握手功能，一次QUIC连接建立可以实现使用 0-RTT 或者 1-RTT 来建立连接。</p><p>UIC在握手过程中使用Diffie-Hellman算法来保证数据交互的安全性并合并了它的加密和握手过程来减小连接建立过程中的往返次数。</p><blockquote><p>Diffie–Hellman (以下简称DH)密钥交换是一个特殊的交换密钥的方法。它是密码学领域内最早付诸实践的密钥交换方法之一。DH可以让双方在完全缺乏对方(私有)信息的前提条件下通过不安全的信道达成一个共享的密钥。此密钥用于对后续信息交换进行对称加密。</p></blockquote><p>QUIC 连接的建立整体流程大致为：QUIC在握手过程中使用Diffie-Hellman算法协商初始密钥，初始密钥依赖于服务器存储的一组配置参数，该参数会周期性的更新。初始密钥协商成功后，服务器会提供一个临时随机数，双方根据这个数再生成会话密钥。客户端和服务器会使用新生的的密钥进行数据加解密。</p><p>以上过程主要分为两个步骤：初始握手（Initial handshake）、最终（与重复）握手（Final (and repeat) handshake），分别介绍下这两个过程。</p><p><strong>初始握手（Initial handshake）</strong></p><p>在连接开始建立时，客户端会向服务端发送一个打招呼信息，（inchoate client hello (CHLO)），因为是初次建立，所以，服务端会返回一个拒绝消息（REJ），表明握手未建立或者密钥已过期。</p><p><img src="https://pic2.zhimg.com/80/v2-61ae95afd407dc8097856542c0c88cfd_720w.webp" alt="img"></p><p>但是，这个拒绝消息中还会包含更多的信息（配置参数），主要有：</p><ul><li>Server Config：一个服务器配置，包括服务器端的Diffie-Hellman算法的长期公钥（long term Diffie-Hellman public value）</li><li>Certificate Chain：用来对服务器进行认证的信任链</li><li>Signature of the Server Config：将Server Config使用信任链的叶子证书的public key加密后的签名</li><li>Source-Address Token：一个经过身份验证的加密块，包含客户端公开可见的IP地址和服务器的时间戳。</li></ul><p>在客户端接收到拒绝消息（REJ）之后，客户端会进行数据解析，签名验证等操作，之后会将必要的配置缓存下来。</p><p>同时，在接收到REJ之后，客户端会为这次连接随机产生一对自己的短期密钥（ephemeral Diffie-Hellman private value） 和 短期公钥（ephemeral Diffie-Hellman public value）。</p><p>之后，客户端会将自己刚刚产生的短期公钥打包一个Complete CHLO的消息包中，发送给服务端。这个请求的目的是将自己的短期密钥传输给服务端，方便做前向保密。</p><p>在发送了Complete CHLO消息给到服务器之后，为了减少RTT，客户端并不会等到服务器的响应，而是立刻会进行数据传输。</p><p>为了保证数据的安全性，客户端会自己的短期密钥和服务器返回的长期公钥进行运算，得到一个初始密钥（initial keys）。</p><p>有了这个初识密钥之后，客户端就可以用这个密钥，将想要传输的信息进行加密，然后把他们安全的传输给服务端了。</p><p>另外一面，接收到Complete CHLO请求的服务器，解析请求之后，就同时拥有了客户端的短期公钥和自己保存的长期密钥。这样通过运算，服务端就能得到一份和客户端一模一样的初始密钥（initial keys）。</p><p>接下来他接收到客户端使用初始密钥加密的数据之后，就可以使用这个初始密钥进行解密了，并且可以将自己的响应再通过这个初始密钥进行加密后返回给客户端。</p><p><strong>所以，从开始建立连接一直到数据传送，只消耗了初始连接连接建立的 1 RTT</strong></p><h2 id="最终（与重复）握手"><a href="#最终（与重复）握手" class="headerlink" title="最终（与重复）握手"></a><strong>最终（与重复）握手</strong></h2><p>那么，之后的数据传输就可以使用初始密钥（initial keys）加密了吗？</p><p>其实并不完全是，因为初始密钥毕竟是基于服务器的长期公钥产生的，而在公钥失效前，几乎多有的连接使用的都是同一把公钥，所以，这其实存在着一定的危险性。</p><p>所以，为了达到前向保密 (Forward Secrecy) 的安全性，客户端和服务端需要使用彼此的短期公钥和自己的短期密钥来进行运算。</p><blockquote><p>在密码学中，前向保密（英语：Forward Secrecy，FS）是密码学中通讯协议的安全属性，指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。</p></blockquote><p>那么现在问题是，客户端的短期密钥已经发送给服务端，而服务端只把自己的长期密钥给了客户端，并没有给到自己的短期密钥。</p><p>所以，服务端在收到Complete CHLO之后，会给到服务器一个server hello(SHLO)消息，这个消息会使用初始密钥（initial keys）进行加密。</p><p><img src="https://pic3.zhimg.com/80/v2-593da9ad871376e1b9c94104ec560172_720w.webp" alt="img"></p><p>这个CHLO消息包中，会包含一个服务端重新生成的短期公钥。</p><p>这样客户端和服务端就都有了对方的短期公钥（ephemeral Diffie-Hellman public value）。</p><p>这样，客户端和服务端都可以基于自己的短期密钥和对方的短期公钥做运算，产生一个仅限于本次连接使用的前向保密密钥 (Forward-Secure Key)，后续的请求发送，都基于这个密钥进行加解密就可以了。</p><p>这样，双方就完成了最终的密钥交换、连接的握手并且建立了QUIC连接。</p><p>当下一次要重新创建连接的时候，客户端会从缓存中取出自己之前缓存下来的服务器的长期公钥，并重新创建一个短期密钥，重新生成一个初始密钥，再使用这个初始密钥对想要传输的数据进行加密，向服务器发送一个Complete CHLO 请求即可。这样就达到了0 RTT的数据传输。</p><p><strong>所以，如果是有缓存的长期公钥，那么数据传输就会直接进行，准备时间是0 RTT</strong></p><p><strong>以上，通过使用Diffie-Hellman算法协商密钥，并且对加密和握手过程进行合并，大大减小连接过程的RTT ，使得基于QUIC的连接建立可以少到1 RTT甚至0 RTT。</strong></p><p><img src="https://pic4.zhimg.com/80/v2-cc893cfb594e784cc21cc82cb7330df3_720w.webp" alt="img"></p><h2 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a><strong>多路复用</strong></h2><p>基于TCP的协议实现的HTTP有一个最大的问题那就是队头阻塞问题，那么，在这方面，QUIC是如何解决这个问题的呢？</p><p>TCP传输过程中会把数据拆分为一个个按照顺序排列的数据包，这些数据包通过网络传输到了接收端，接收端再按照顺序将这些数据包组合成原始数据，这样就完成了数据传输。</p><p>但是如果其中的某一个数据包没有按照顺序到达，接收端会一直保持连接等待数据包返回，这时候就会阻塞后续请求。这就发生了TCP队头阻塞。</p><p>类似于HTTP&#x2F;2，<strong>QUIC在同一物理连接上可以有多个独立的逻辑数据流，这些数据流并行在同一个连接上传输，且多个数据流之间间的传输没有时序性要求，也不会互相影响。</strong></p><blockquote><p>数据流（Streams）在QUIC中提供了一个轻量级、有序的字节流的抽象化</p></blockquote><p>QUIC的单个数据流可以保证有序交付，但多个数据流之间可能乱序。这意味着单个数据流的传输是按序的，但是多个数据流中接收方收到的顺序可能与发送方的发送顺序不同！</p><p>也就是说同一个连接上面的多个数据流之间没有任何依赖（不要求按照顺序到达），即使某一个数据包没有达到，也只会影响自己这个数据流，并不会影响到到其他的数据流。</p><h2 id="连接迁移你"><a href="#连接迁移你" class="headerlink" title="连接迁移你"></a><strong>连接迁移你</strong></h2><p>对于TCP连接的识别，需要通过服务器和客户端过双方的ip和端口四个参数进行的。在网络切换的场景中，比如手机切换网络，那么自身的ip就会发生变化。</p><p>这就导致之前的TCP连接就会失效，就需要重新建立。这种场景对于移动端设备普及的今天来说，还是比较频繁的。</p><p>所以，在这一点上，QUIC进行了优化。</p><p><strong>QUIC协议使用特有的UUID来标记每一次连接，在网络环境发生变化的时候，只要UUID不变，就能不需要握手，继续传输数据。</strong></p><h2 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a><strong>可靠性</strong></h2><p>TCP之所以被称之为可靠链接，不仅仅是因为他有三次握手和四次关闭的过程，还因为他做了很多诸如<strong>流量控制、数据重传、拥塞控制</strong>等可靠性保证。这</p><p>也是为什么一直以来都是以TCP作为HTTP实现的重要协议的原因。</p><p>那么，QUIC想要取代TCP，就需要在这方面也做出努力，毕竟UDP自身是不具备这些能力的。</p><p>TCP拥塞控制是TCP避免网络拥塞的算法，是互联网上主要的一个拥塞控制措施。经典的算法实现有很多，诸如TCP Tahoe 和 Reno、TCP Vegas、TCP Hybla、TCP New Reno、TCP Westwood和Westwood+以及TCP BIC 和 CUBIC等等。</p><p>QUIC协议同样实现了<strong>拥塞控制</strong>。不依赖于特定的拥塞控制算法，并且提供了一个可插拔的接口，允许用户实验。默认使用了 TCP 协议的 Cubic 拥塞控制算法。</p><p>关于<strong>流量控制</strong>，QUIC提供了基于stream和connection两种级别的流量控制，既需要对单个 Stream 进行控制，又需要针对所有 Stream 进行总体控制。</p><p>QUIC的连接级流控，用以限制 QUIC 接收端愿意分配给连接的总缓冲区，避免服务器为某个客户端分配任意大的缓存。连接级流控与流级流控的过程基本相同，但转发数据和接收数据的偏移限制是所有流中的总和。</p><h2 id="弊端"><a href="#弊端" class="headerlink" title="弊端"></a><strong>弊端</strong></h2><p>以上，我们介绍了很多QUIC的相比较于TCP的优点，可以说这种协议相比较于TCP确实要优秀一些。</p><p>因为他是基于UDP的，并没有改变UDP协议本身，只是做了一些增强，虽然可以避开中间设备僵化的问题，但是，在推广上面也不是完全没有问题的。</p><p>首先，<strong>很多企业、运营商和组织对53端口（DNS）以外的UDP流量会进行拦截或者限流</strong>，因为这些流量近来常被滥用于攻击。</p><p>特别是一些<strong>现有的UDP协议和实现易受放大攻击（amplification attack）威胁，攻击者可以控制无辜的主机向受害者投放发送大量的流量。</strong></p><p>所以，基于UDP的QUIC协议的传输可能会受到屏蔽。</p><p>另外，因为UDP一直以来定位都是不可靠连接，所以有很多中间设备对于他的支持和优化程度并不高，所以，出现丢包的可能性还是比较高的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计网 </tag>
            
            <tag> http </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7.1~7.2前往保定考试旅途随笔</title>
      <link href="/2024/05/02/7-1-7-2%E5%89%8D%E5%BE%80%E4%BF%9D%E5%AE%9A%E8%80%83%E8%AF%95%E6%97%85%E9%80%94%E9%9A%8F%E7%AC%94/"/>
      <url>/2024/05/02/7-1-7-2%E5%89%8D%E5%BE%80%E4%BF%9D%E5%AE%9A%E8%80%83%E8%AF%95%E6%97%85%E9%80%94%E9%9A%8F%E7%AC%94/</url>
      
        <content type="html"><![CDATA[<h1 id="7-1-7-2前往保定考试旅途随笔"><a href="#7-1-7-2前往保定考试旅途随笔" class="headerlink" title="7.1~7.2前往保定考试旅途随笔"></a>7.1~7.2前往保定考试旅途随笔</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今年三月份，我报名参加了日语N2考试。由于考生众多，我没有成功选择到山大作为考点。而且实际上我没有花太多时间来学习，大部分时间都在复习之前学过的内容。这次考试我没有太多底气，但我仍然决定去参加。毕竟N2考试的费用是550块钱，不去的话会觉得很心疼，去了也同样如此。因为七月三号还要参加高数考试，匆忙从保定赶回来再去考试简直是一场灾难。然而，在经过一番思想斗争后，我决定还是去保定参加N2考试。</p><h2 id="旅途"><a href="#旅途" class="headerlink" title="旅途"></a>旅途</h2><h4 id="出发前"><a href="#出发前" class="headerlink" title="出发前"></a>出发前</h4><p>我提前购买了下午三点的火车票。七月份正值放假季节，中考和高考也已结束，大部分大学也陆续放假了（为什么太理放假这么晚呢&#x3D; &#x3D;）。因此，火车票并不容易购买。七月一日中午十二点，我吃完了食堂里六块半的经济套餐，然后回到宿舍整理行李。现代基础设施的完善减少了旅途中的很多麻烦，所以我只带了身份证、手机、N2语法书，还有一些路上吃的食品和一瓶水。在宿舍里闲逛了一阵后，我一点半就前往公交车站准备去太原南站。</p><h4 id="公交车上"><a href="#公交车上" class="headerlink" title="公交车上"></a>公交车上</h4><p>公交车上没有以往星期天那种挤得水泄不通的情景了，大学城的学校基本上已经放假，所以公交车相当空旷。这是我第一次坐上如此空旷的902路公交车，车上只有寥寥十几个人，都是学生，看起来和我一样的很多人是前往南站或太原站的。也许还有一些人是去太原的吧。车内的宽敞空间让我感到非常舒适。公交车在一次次的停靠和行驶中，很快我就到达了南站。</p><h4 id="在南站"><a href="#在南站" class="headerlink" title="在南站"></a>在南站</h4><p>我曾在南京游玩，回程时在南站下车。然而，当时已是晚上九点，南站笼罩在漆黑之中，一片朦胧中无法分辨周围景象。疲惫的旅途使我只想尽快返回学校，无暇顾及南站的容貌。这次我有时间来欣赏南站的建筑风格了。南站前面是一个公园，有许多条蜿蜒曲折的小路，有些人为的痕迹，美感并不十分突出，反而有些多余之感。南站前方是一个广场，中国的火车站广场通常都非常宽广，以应对大量的人流。广场的宽阔程度在一定程度上反映了这个地方的客流量。与吞吐大站如北京站相比，南站和太原站的广场并不宽敞，这说明南站和太原站的客流量并不大。我背着背包走进入站口时，看到武警整齐地在广场上巡逻。广场上还设有许多警亭，显然火车站这种人流众多、人员杂乱的地方存在着安全事故的潜在风险。火车站历来都是诈骗和欺诈的热门地，当然现在随着互联网的兴起，即使在陌生地方，人们也可以通过互联网找到信息，减少了人与人之间的交流，从某种程度上遏制了这类案件的发生。在当前个人社交关系日益碎片化的情况下，人与人之间的交流越来越少，也没有很强的人际联系，因此诈骗的机会也减少了。</p><p>我通过安检进入火车站时已经是两点十几分，火车出发还有一个多小时。我闲暇之余开始观察火车站的结构以及各式各样的人们。太原火车站的候车厅有三层，第一层主要用于安检，并不是主要的候车区域，周围设有厕所和供应热水的房间。热水可以泡面，也可作为饮料，可谓旅途中的必需品。而二层则是主要的候车厅。候车厅内的IED屏幕上显示着各个列车的发车时间和检票站台，工作人员在这里维持秩序并提醒旅客检票。而三层则是各式餐厅，为人们提供丰富的饮食选择，满足他们的渴望填饱肚子的需求。这些餐厅提供各种口味的美食，无论是中餐还是西餐，都能满足不同人的口味偏好。人们可以在这里享受美食，放松身心，为接下来的旅程补充能量。餐厅区域的存在让人们在火车站内不必为饥饿而担忧，提供了方便和便利。</p><p>火车站的人流最能真实地反映当下中国的精神风貌，与网络上所呈现的不同。在车站来来往往的人们中，他们的经历最真实且具有代表性。车站里有返乡的大学生，手拎大包小包的毕业生，还有刚结束高考的学生们外出旅游放松心情。还有灰头土脸的中年男女，看起来像是工地上的工人，他们经常成群结队，互相照应，还有穿着衬衫的中年男士，看起来像是出差。他们代表着中国的新生代和老一代，是两个时代的交汇点。新生代年轻人充满朝气和活力，他们拥抱着现代科技和潮流文化，追求个人价值和自由选择。他们对未来充满希望，并努力追寻自己的梦想。而老一代人则承载着历史的沉淀和智慧的积累。他们经历过风雨，见证了国家的发展和变革。这两代人之间的交流和碰撞，代表着中国社会的多样性和变迁。他们相互启迪，互相影响，共同构建着一个更加开放、包容和进步的社会。他们的共同存在展示了中国社会的丰富性和活力，也为国家的未来发展注入了新的动力和希望。在一圈圈闲逛之中，时间到了三点二十分，开往保定的列车开始检票，我随着人流，走向检票口，正式前往去保定的旅程。</p><h4 id="火车上"><a href="#火车上" class="headerlink" title="火车上"></a>火车上</h4><p>虽然我有心理准备，但刚一上火车就被火车上人员的密集程度深深震撼了。有一种我开学坐绿皮慢火车到学校的既视感。我寻找到我的座位，自言自语地念着座位号24号……突然，面前的一位中年男人告诉我，24号在他旁边。我是25号，所以我旁边就是24号。他直直地看着前方，并没有看向我，但我并没有产生什么疑惑。他站起身让我进去坐下，我的对面坐着一位小妹妹，正在专心地看着一本小说。从外表看，她大约十六七岁的样子。把包放到脚下后我开始和那位中年男人攀谈起来。</p><p>我问他：“您是要去哪儿？”他回答说他要去北京的一家按摩店工作。一提到按摩店，我就想到通常会有一个定语——盲人。然后他告诉我，他是一位视障人士，正如我所预料的。回想起来，他的眼神和行为确实符合一个盲人的特征。我好奇地询问他是先天性还是后天性的视力问题，他回答说是后天的。他从小眼睛就不太好，但还能看清楚东西。然而，随着年龄增长，他的视力状况逐渐恶化。他的家人曾带他到各个医院，但现代医学对于他的症状无能为力。到了他三十岁时，他的视力已经恶化到只剩下光感。相比完全失明，他还能感受到光的存在。然而，他的世界一片黑暗，只能辨别一些光线差距较大的物体。但在相对昏暗或没有明暗差异的环境中，他就无法辨别了，所以他经常在楼梯和电线杆等地方碰撞。只有经过多次碰撞后，他才能记住这些物体的位置，所以他在熟悉的地方仍然能够顺利行走。</p><p>他曾经做了很长时间的销售工作，到过很多地方，如大同、河南、河北、天津、川渝地区，可以说是走遍了北方。然而，随着视力恶化，他无法继续从事销售工作。于是，他报名参加了太原的一所特殊教育学校。在学校里，他学习了许多关于人体构造和生理学等知识。接近四十岁的年龄，他在学校已经成为了学生中的长辈，受到了同学们的尊敬。这次也是在暑假的时候去打暑假工，赚一些钱。他早已成家，有一个七岁的女儿。所以这次也是打工赚钱减轻家中负担，毕竟读书可是没有收入来源的。说到他的女儿，他的脸上露出了幸福的神情，和我讲他女儿的事情，说平时虽然压力很大，但是他女儿扑到他身上叫爸爸的时候，他觉得一切压力在那一刻就好像烟消云散了。</p><p>接着他又讲了许多在学校生活的事情。通过他的讲述，我对特殊教育学校有了更多的了解。这是我第一次面对面的听残障人士讲述他们的日常生活。学校的学生都是残障人士，他们也要学习各种课程，试卷使用盲文打印。他们也为考试而担忧。除了身体上的缺陷，他们与普通学生没有太大区别。如果要说有什么不同，那就是他们的内心非常坚强。面对后天的残疾，要克服这种差距感需要很大的勇气。有很多人像史铁生一样变得暴躁和自暴自弃，但他们仍然会在自己的赛道上努力学习，发挥自己的能力，努力生活。</p><p>我们正谈得起劲时，第二站上上来一位中年妇女，坐在我们对面，加入了我们的谈话。她的丈夫在北京的工地上工作，因受了些工伤需要她去照料。他们开始交谈，我也插不上什么话，于是我拿出平板电脑和语法书，临时抱佛脚地复习N2的语法题。在他们谈论的背景声中，火车抵达了下一站——阳泉站。这时，一个年轻人上车坐在我旁边，看见我手上的书，惊讶地说：“你也是去保定考试的吗？”我回答说：“没错。”他将他的包放下，从包里拿出一份N2的真题开始做。在我一次次翻书的过程中，旁边的中年人询问我：“你在看书吗？”我哈哈一声说：“我在临时抱佛脚。”也是因此我们开始聊起现代学生们所面临的压力等话题。在这个过程中，我也开始描述我在学校看到的各种情景，向他们展示我身边的学霸们每天在学校的作息，以及我们每天学习的课程，以及对未来的规划。谈到规划时，我们又谈论起大学教育。我提到大学需要大量的资金来支持各种研究，而山西省的教育由于资金的缺陷发展受到一定的局限性，导致山西的高等教育相对落后。与之相比，南方的一些高校资金充足，能够有更好的发展。我还分享了我去南京游玩时所看到的与山西完全不同的景色，以及我在南京大学门口参观留念的经历。当提到南京大学时，坐在旁边的年轻人突然插话说：“你去的是哪个校区？”我回答说：“鼓楼校区。”他说：“那是我的学校。”我感到非常惊讶，疑惑地问道：“你是放假回家了吗？”他回答说他是今年考上南京大学研究生的，专业是动物学。我望着他，心生羡慕之情，说道：“我也想去南京大学，南京大学的人工智能学科建设得非常好，而且南京的风景优美，气候宜人，是我心中的梦想之地。”他也只是打个哈哈，继续写他的真题，没有多说什么。</p><p>就这样在火车的摇晃中，我们到达了石家庄，距离保定已经不远了。这时我们又开始讨论手机带来的关怀模式，以及如何帮助残障人士使用手机。我们进一步探讨了互联网的现状。我告诉他，现在由于技术框架的成熟，许多功能都可以模块化，以前需要大团队从头开始构建的项目，现在只需要少数几个人就能完成。尤其是人工智能的快速发展，许多公司专注于小而精的领域，例如Midjourney这样的公司，仅有八个人就能打造出市值上亿美元的企业。</p><p>然后，他告诉我他提前通过12306热线申请了残障人士服务，在下车时会有工作人员一直陪同他到地铁站。他还提到，现在国家对残障人士的关怀越来越好，包括他所就读的学校也是免费的。这时我才意识到，其实身边存在许多我们所不了解的细节和关怀。</p><p>在一句句的对话中，时间过得非常快。旁边的年轻小哥突然问我是否已经预订了酒店，我回答说已经提前预订好了。我们一起研究了考试地点和酒店，决定在下车后一起行动。最后阶段，坐在对面的小妹也加入了我们的对话。她在太原的一所学校学习材料学，但不愿透露学校的名字。</p><p>学历、学校、技术栈、实习和论文等。这些事情不仅压在我们身上，同时也疏远了人与人之间的距离。这是一个既好又坏的时代，在市场经济的大背景下，我们拥有极高的自由度，可以选择自己希望从事的职业。不再需要像父辈那样长时间积累经验，被限定在某个框框里。然而，自由也带来了竞争的激烈。好的资源是每个人都渴望得到的，稍有松懈就可能被他人夺走自己的位置。那位小妹选择沉默，应该也不例外是因为这些原因。</p><p>我们到达保定站后，整理好行李，与一路上的中年男性告别，他祝愿我们考试顺利。我笑了笑，也祝他在北京的工作一切顺利。虽然只是短短三个小时的交谈，但说实话，还是有些不舍。但旅途就是这样，相逢又相离。每个人都有自己要去完成的事情，旅途只是暂时将我们聚集在一起，之后我们都要投身于自己的事务中去。</p><h4 id="在保定"><a href="#在保定" class="headerlink" title="在保定"></a>在保定</h4><p>一下火车，我就感受到了保定扑面而来的热浪。我原本以为太原已经很热了，没想到保定更甚。出了保定站，我和小哥一起打车去他的宾馆，而我的宾馆离他并不远。在路上，我们开始讨论晚餐的问题。我提到保定有名的驴肉火烧，我们是否要去找一家店尝一尝。他也表示同意。我向司机师傅询问附近是否有好吃的驴肉火烧店，但司机告诉我们这附近没有什么好的地方，而且由于时间紧迫，我们也无法专门在保定游玩。只好放弃了这个计划，在宾馆附近找了一家卖凉皮的店解决晚餐。吃完饭后互相道别，我朝着我的酒店前进。</p><p>在路上，我观察着保定城区的建设。感觉就像回到了灵丘县一样，一片破旧的景象，建筑风格也相似。这是理所当然的，毕竟灵丘和保定是接壤的地方，而且保定也不是多富裕的地方，相似的风格是不可避免的。</p><p>到达酒店后，我打开空调遮蔽外部的酷热，然后冲了个澡。疲惫的我躺在床上玩了一会手机，向家人报平安后就入睡了。转眼第二天就到了。我在酒店订了外卖的驴肉火烧和馄饨作为午餐。那肥瘦有秩的驴肉火烧，咬下去，外皮酥脆，里面的肉与小麦一起在口中释放出美味。油脂的香气与小麦的香气交织在一起，让味蕾陶醉。吃了两个后仍然回味无穷。</p><p>午饭后，我离开房间前往考场。路上依然炎热难耐，我看到身边也有背着背包的旅客，他们与我同向同行，看来我们有着相同的目的。走了大约半小时，我们到达一个学校门口，许多背着背包的年轻人聚集在这里，显然这就是考场所在地。尽管大家年龄、性别各异，但我们都透露着一股宅气。我身穿一件IEM冠军T恤，有人认出我，向我说理赔难牛逼，我笑了笑。还有人跟我攀谈，表示紧张，我安慰他说没事放松心态，我也只是凭着N2.5的水平去考N2，放宽心。</p><h2 id="考试"><a href="#考试" class="headerlink" title="考试"></a>考试</h2><p>到了考试时间，大门打开，工作人员开始检查准考证和身份证。这所小学也是非常破旧的，我跟着人群找到了我的考场。炎热的天气让大家都汗流浃背，边拿东西扇风边走向考场。由于考场是在小学里，座椅都非常矮小，我作为一米八的个子坐在那里非常拘束和不便。我观察着考场上的每个人，有几个看起来像初高中生的人，也有像我一样的大学生，甚至还有一位大约四十多岁的中年妇女也在考场上。</p><p>教室里有几个电扇，所以考场上的温度并没有太难受（比太理还好一些）。在大喇叭里播放了各种提示和注意事项后，考试开始了。我拿到了一本被定成小册子一样的卷子，我迅速完成了文法和单词部分，然后开始做阅读理解。题目并不是特别难，但我之前没有做过这种类型的题，推进速度很慢。到后面越来越紧张，阅读理解的题目实在太多，而我阅读速度并不快，结果我最终没有完成，有三个选择题是瞎蒙的。</p><p>文法测验结束后是听力测验，休息了十分钟后，在一声声“天気がいいから、散歩しましょう”（因为天气好，我们去散步吧）的提示下，听力测试开始了。由于我的听力不好，基本上也是半蒙半做。考试结束后，我感到如释重负，走出考场去和小哥会和。人数非常多，而且还出现了交通拥堵，我们花了很多时间才打上车前往保定东站。他要去山东，而我急于赶往太原。在东站我们分别，各自奔向自己的旅途。</p><h2 id="归途"><a href="#归途" class="headerlink" title="归途"></a>归途</h2><p>回程的路上并没有什么特别引人注意的事情，非常平淡。只有在站台等车的时候，被红金色车厢的复兴号速度所震撼。在点外卖的时候，我多点了几个火烧，打算带回去分给朋友。为了考试，我在动车上看高数的课程和真题。高铁有空调和插座，比火车舒适许多。不知不觉中，我已经到达了太原。</p><p>在太原南站，我在地下兜兜转转找到了一个网约车停车场。南站的地下布局非常复杂，我打车后，尽管司机离我不远，我仍然不知道应该去哪里找他。只能等司机来接我。司机绕了一个大圈子才找到我，但态度非常和蔼。他询问我是否刚刚接到了订单，我回答说是的。他告诉我他家在榆次，每天七点多就不接单了，而我恰好是他最后一单。在路上，我逐渐熟悉的景色出现了，我回到了学校。司机说谢谢乘车，我也回答着小心路程。回到学校后，我把火烧分给了朋友们，然后立即投入到紧张的复习中。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>回顾这次前往保定的旅程，我不禁感慨万分。在这短短的两天里，我遇到了许多有趣而特别的人，聆听了他们的故事，分享了彼此的经历和见解。旅途中的火车车厢、公交车、考场和宾馆都成为了我们相聚和别离的见证。每一个细节都彰显着生活的多样性和奇妙之处。</p><p>通过与那位视障人士的交谈，我对特殊教育学校的理解更加深刻，也对身体有缺陷的人们充满了敬意。他们面对挑战，勇敢面对生活，为自己创造机会，证明了坚持和努力的重要性。与那位中年男性的对话让我更加珍惜现在的自由和机会，也让我深刻认识到竞争的激烈和自身的努力不可或缺。</p><p>这次旅程不仅让我体验到了不同城市的风景和氛围，也拓宽了我的眼界，让我更加明白自己的责任和使命。作为新生代的一员，我们要学习和掌握更多的知识，积极面对挑战，不断提升自己的能力和素养。同时，我们也应该尊重和关注他人的需求和感受，为社会的进步和发展贡献自己的力量。</p><p>旅途结束，我回到学校，投入到紧张的复习中。这段经历成为了我人生中的一段美好回忆，激励着我继续努力追求自己的目标。无论是旅途中的相遇还是考试中的挑战，都让我更加坚定了自己的信念和决心。我相信，只要不断努力和奋斗，我们一定能够迎接更加美好的未来。</p><p>愿每一个旅程都成为人生的瑰宝，让我们在不同的路上不断成长，收获智慧和勇气，让我们的人生更加丰富多彩。感谢这次旅程给我带来的一切，我将珍藏这段经历，继续向前迈进。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门</title>
      <link href="/2024/05/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
      <url>/2024/05/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习入门"><a href="#深度学习入门" class="headerlink" title="深度学习入门"></a>深度学习入门</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机接受两个输入信号。<em>x</em>1、<em>x</em>2是输入信号，<em>y</em>是输出信号，<em>w</em>1、<em>w</em>2是权重（<em>w</em>是weight的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（<em>w</em>1<em>x</em>1、<em>w</em>2<em>x</em>2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号<em>θ</em>表示。</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231215155935558.png" alt="image-20231215155935558"></p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>将输入信号总和转换为输出信号的就是激活函数<br>$$<br>a &#x3D; b + w_1 x_1 + w_2x_2<br>$$</p><h5 id=""><a href="#" class="headerlink" title=""></a></h5><p>$$<br>y &#x3D; h(a)<br>$$</p><h5 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h5><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231215160255333.png" alt="image-20231215160255333"></p><p>神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元</p><h5 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h5><p>分类问题中使用的softmax函数可</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231215161608917.png" alt="image-20231215161608917"></p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231215161514935.png" alt="image-20231215161514935"></p><h4 id="softmax函数的特征"><a href="#softmax函数的特征" class="headerlink" title="softmax函数的特征"></a>softmax函数的特征</h4><p>softmax函数的输出是0*.<em>0到1</em>.*0之间的实数输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax函数的输出解释为“概率”。</p><h2 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h2><h4 id="神经网络的学习步骤"><a href="#神经网络的学习步骤" class="headerlink" title="神经网络的学习步骤"></a>神经网络的学习步骤</h4><p>步骤<strong>1</strong>（<strong>mini-batch</strong>）</p><p>从训练数据中随机选择一部分数据。</p><p>步骤<strong>2</strong>（计算梯度）</p><p>计算损失函数关于各个权重参数的梯度。</p><p>步骤<strong>3</strong>（更新参数）</p><p>将权重参数沿梯度方向进行微小的更新。</p><p>步骤<strong>4</strong>（重复）</p><p>重复步骤1、步骤2、步骤3。</p><p>神经网路的特征就是线虫数据中学习，也就是由数据去决定各个参数的值，在神经网络中各种参数的值会是成千上万的，在这种情况下，由人工去决定机器的参数是不现实的事情。</p><p>手写数字识别是深度学习的一个经典案例，每个人写出来的5都是不同的，那么怎么样才能识别一个字是不是五呢，</p><p>从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。一种方案是，先从图像中提取特征量再用机器学习技术学习这些特征量的模式。机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始</p><p>想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担</p><h4 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h4><p>我们通过训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。只对某个数据集过度拟合的状态称为过拟合（over fitting）。避免过拟合也是机器学习的一个重要课题</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p><h5 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h5><p>$$<br>E &#x3D; \frac{1}{2} \sum_{k}^{}(yk-tk)^2<br>$$</p><p>$$<br>yk是表示神经网络的输出，tk表示监督数据，k表示数据的维数<br>$$</p><h5 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h5><p>交叉熵误差（cross entropy error）也经常被用作损失函数。交叉熵误差如下式所示。</p><p>交叉熵是用来评估当前训练得到的<strong>概率分布</strong>与真实分布的差异情况。 它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。<br>$$<br>E &#x3D;  -\sum_{k}^{}t_k\log_{}{y_k}<br>$$</p><h3 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h3><p>$$<br>E &#x3D; -\frac{1}{N}\sum_{n}^{} \sum_{k}^{}t_{nk}\log_{}{y_{nk}}<br>$$</p><p>假设数据有<em>N</em>个，N表示第<em>n</em>个数据的第<em>k</em>个元素的值（$ $是神经网络的输出，tnk是监督数据）</p><h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>$$<br>f(x_0,x_1) &#x3D; x_0^2+x_1^2<br>$$</p><p>这个函数有两个参数式（4*.<em>6）有两个变量，所以有必要区分对哪个变量求导数，即对</em>x<em>0和</em>x*1两个变量中的哪一个求导数。另外，我们把这里讨论的有多个变量的函数的导数称为偏导数。用数学式表示的话，可以写成 $\frac{\partial f}{\partial x_0} $、$\frac{\partial y}{\partial x_1} $</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>像 $\frac{\partial f}{\partial x_0} $、$\frac{\partial y}{\partial x_1} $​这样的由全部变量的偏导数汇总而成的向量称为梯度（gradient）。梯度指示的方向是各点处的函数值减小最多的方向 。</p><h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>深度学习需要找到最优参数（权重与偏置），但是损失函数通常非常的繁杂，无法通过人工计算的方式来做优化，于是我们使用梯度下降的方式来减少损失函数</p><p>但是梯度下降的方式可能不会达到一个全局最优解 而是陷入一个局部的最优解。虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。</p><p>此时梯度法就派上用场了。在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。<br>$$<br>x_0 &#x3D;x_0-\eta\frac{\partial f}{\partial x_1}<br>$$</p><p>$$<br>x_1 &#x3D;x_1-\eta\frac{\partial f}{\partial x_1}<br>$$</p><p><em>η</em>表示更新量，在神经网络的学习中，称为学习率（learning rate）。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。</p><h3 id="随机梯度下降法（stochastic-gradient-descent）"><a href="#随机梯度下降法（stochastic-gradient-descent）" class="headerlink" title="随机梯度下降法（stochastic gradient descent）"></a>随机梯度下降法（stochastic gradient descent）</h3><p>“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法</p><h2 id="误差反向传播法"><a href="#误差反向传播法" class="headerlink" title="误差反向传播法"></a>误差反向传播法</h2><h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示(其实就是从里向外一层一层算)</p><h4 id="反向传播法"><a href="#反向传播法" class="headerlink" title="反向传播法"></a>反向传播法</h4><p>相反与正向传播 反向传播是乘以对应节点的导数</p><p>以$z &#x3D; x + y$为例其导数分别为<br>$$<br>\frac{\partial z}{\partial x}&#x3D;1<br>$$</p><p>$$<br>\frac{\partial z}{\partial y}&#x3D;1<br>$$</p><p>因此加法节点的反向传播只是将输入信号输出到下一个节点</p><h5 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h5><p>以$z&#x3D;xy$为例<br>$$<br>\frac{\partial z}{\partial x}&#x3D;y<br>$$</p><p>$$<br>\frac{\partial z}{\partial y}&#x3D;x<br>$$</p><p>乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p>确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）</p><h2 id="与学习相关的技巧"><a href="#与学习相关的技巧" class="headerlink" title="与学习相关的技巧"></a>与学习相关的技巧</h2><h4 id="参数的更新"><a href="#参数的更新" class="headerlink" title="参数的更新"></a>参数的更新</h4><p>神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。</p><h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>和前面的SGD一样，<strong>W</strong>表示要更新的权重参数， 表示损失函数关于<strong>W</strong>的梯度，<em>η</em>表示学习率。这里新出现了一个变量<strong>v</strong>，对应物理上的速度。</p><h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>学习率作为一种超参数十分重要在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。</p><p>AdaGrad会为参数的每个元素适当地调整学习率，</p><p>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。</p><h4 id="权重的初始值"><a href="#权重的初始值" class="headerlink" title="权重的初始值"></a>权重的初始值</h4><p>在神经网络的学习中，权重的初始值特别重要。设定什么样的权重初始值，经常关系到神经网络的学习能否成功。</p><p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途</p><p>数据分布造成反向传播中梯度的值不断变小，最后消失。这个问题称为梯度消失（gradient vanishing）</p><p>Xavier初始值</p><p>如果前一层的节点数为<em>n</em>，则初始值使用标准差为 $\frac{1}{\sqrt{n}}$的分布</p><h5 id="ReLU的权重初始值"><a href="#ReLU的权重初始值" class="headerlink" title="ReLU的权重初始值"></a>ReLU的权重初始值</h5><p>Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也就是Kaiming He等人推荐的初始值，也称为“He初始值。</p><p>当前一层的节点数为<em>n</em>时，He初始值使用标准差为 $\sqrt\frac{2}{n}$的高斯分布。当Xavier初始值是$\sqrt\frac{1}{n}$时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数</p><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>机器学习的问题中，过拟合是一个很常见的问题。</p><p>发生过拟合的原因，主要有以下两个。</p><p>• 模型拥有大量参数、表现力强。</p><p>• 训练数据少</p><h4 id="权值衰减"><a href="#权值衰减" class="headerlink" title="权值衰减"></a>权值衰减</h4><p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。</p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情况下，我们经常会使用Dropout方法</p><p>Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出</p><h4 id="超参数的验证"><a href="#超参数的验证" class="headerlink" title="超参数的验证"></a>超参数的验证</h4><p>超参数是指，比如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值，模型的性能就会很差。</p><h5 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h5><p>之前使用的数据集分成了训练数据和测试数据，训练数据用于学习，测试数据用于评估泛化能力。由此，就可以评估是否只过度拟合了训练数据(是否发生了过拟合），以及泛化能力如何等。</p><p>如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。</p><p>所以调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。我</p><h4 id="超参数的最优化"><a href="#超参数的最优化" class="headerlink" title="超参数的最优化"></a>超参数的最优化</h4><p>进行超参数的最优化时，逐渐缩小超参数的“好值”指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。</p><h6 id="最优化的步骤："><a href="#最优化的步骤：" class="headerlink" title="最优化的步骤："></a>最优化的步骤：</h6><ol><li>设定超参数的范围。</li><li>从设定的超参数范围中随机采样。</li><li>使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。</li><li>重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。</li></ol><h2 id="卷积神经网络（Convolutional-Neural-Network，CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network，CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network，CNN）"></a>卷积神经网络（Convolutional Neural Network，<strong>CNN</strong>）</h2><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><h5 id="全连接层存在的问题"><a href="#全连接层存在的问题" class="headerlink" title="全连接层存在的问题"></a>全连接层存在的问题</h5><p>相邻层的所有神经元之间都有连接，这称为全连接（fully-connected）。</p><p>全连接层存在数据的形状被忽视的问题</p><p>比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。</p><p>图像是3维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，</p><p>而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，可以（有可能）正确理解图像等具有形状的数据</p><p>CNN 中，有时将卷积层的输入输出数据称为<strong>特征图（feature map）</strong>。其中，卷积层的输入数据称为<strong>输入特征图（input feature map）</strong>，输出数据称为<strong>输出特征图（output feature map）</strong>。</p><h5 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h5><p>对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。这里所说的窗口是指图7-4中灰色的3 <em>×</em> 3的部分。如图7-4所示，将各个位置上卷积核的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。</p><h5 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h5><p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充（padding）</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231227170322471.png" alt="image-20231227170322471"></p><p>卷积运算的填充处理：向输入数据的周围填入<strong>0</strong>（图中用虚线表示填充，并省略了填充的内容“<strong>0</strong>”）</p><h5 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h5><p>应用滤波器的位置间隔称为步幅（stride）。</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231227170408400.png" alt="image-20231227170408400"></p><p>这里，假设输入大小为(<em>H, W</em>)，滤波器大小为(<em>FH, FW</em>)，输出大小为(<em>OH, OW</em>)，填充为<em>P</em>，步幅为<em>S</em>。此时，输出大小可通过式进行计算。<br>$$<br>OH &#x3D; \frac{H+2P-FH}{S}+1<br>$$</p><p>$$<br>OW &#x3D; \frac{W+2P-FW}{S}+1<br>$$</p><p>其实就是拿这个公式算长和宽</p><h4 id="3维数据的卷积运算"><a href="#3维数据的卷积运算" class="headerlink" title="3维数据的卷积运算"></a>3维数据的卷积运算</h4><p>通道方向上有多个特征图时，会按通道</p><p>进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。</p><p>在3维数据的卷积运算中，输入数据和滤波器的通道要设为相同的值。</p><p>有多高的数据就要有多少个卷积核</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数</p><p>据保存为4维数据。具体地讲，就是按(batch_num*,* channel*,* height*,* width)</p><p>的顺序保存数据。</p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>池化是缩小高、长方向上的空间的运算。</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231227214606929.png" alt="image-20231227214606929"></p><p>就是从多的数据变成少的数据</p><p>除了Max池化之外，还有Average池化等，Average池化则是计算目标区域的平均值，在图像识别领域，主要使用Max池化。</p><h5 id="池化层的特征"><a href="#池化层的特征" class="headerlink" title="池化层的特征"></a><strong>池化层的特征</strong></h5><p>没有要学习的参数池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。</p><p>通道数不发生变化</p><p>经过池化运算，输入数据和输出数据的通道数不会发生变化。</p><p>计算是按通道独立进行的对微小的位置变化具有鲁棒性（健壮）</p><p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。比如，3 <em>×</em> 3的池化的情况下，如图池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）</p><ul><li>CNN在此前的全连接层的网络中新增了卷积层和池化层。</li><li>使用im2col函数可以简单、高效地实现卷积层和池化层。</li></ul><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>深度学习是加深了层的深度神经网络。</p><h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>Data Augmentation是基于算法“人为地”扩充输入图像（训练图像），比如说对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。Data Augmentation还可以通过其他各种方法扩充图像，比如裁剪图像的“crop处理”、将图像左右翻转的“flip处理”A 等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。不管怎样，通过Data Augmentation巧妙地增加训练图像，就可以提高深度学习的识别精度。虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果。</p><h2 id="加深层的好处"><a href="#加深层的好处" class="headerlink" title="加深层的好处"></a>加深层的好处</h2><p>可以减少网络的参数数量。加深了层的网络可以用更少的参数达到同等水平（或者更强）的表现力。一次5 <em>×</em> 5的卷积运算的区域可以由两次3 <em>×</em> 3的卷积运算抵充。并且，相对于前者的参数数量25（5 <em>×</em> 5），后者一共是18（2 <em>×</em> 3 <em>×</em> 3）</p><p>叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大感受野（receptive field，给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将 ReLU等激活函数夹在卷积层的中间，进一步提高了网络的表现力。这是因为向网络添加了基于激活函数的“非线性”表现力，通过非线性函数的叠加，可以表现更加复杂的东西。</p><p>加深层的另一个好处就是使学习更加高效。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效地进行学习。</p><p>实践中经常会灵活应用使用ImageNet这个巨大的数据集学习到的权重数据，这称为迁移学习，将学习完的权重（的一部分）复制到其他神经网络，进行再学习（fine tuning）。比如，准备一个和 VGG相同结构的网络，把学习完的权重作为初始值，以新数据集为对象，进行再学习。迁移学习在手头数据集较少时非常有效。</p><h3 id="基于GPU的高速化"><a href="#基于GPU的高速化" class="headerlink" title="基于GPU的高速化"></a>基于GPU的高速化</h3><p><strong>GPU</strong>计算的目标就是将这种压倒性的计算能力用于各种用途。所谓GPU计算，是指基于GPU进行通用的数值计算的操作。</p><p>深度学习中需要进行大量的乘积累加运算（或者大型矩阵的乘积运算）。这种大量的并行运算正是GPU所擅长的（反过来说，CPU比较擅长连续的、复杂的计算）。因此，与使用单个CPU相比，使用GPU进行深度学习的运算可以达到惊人的高速化。深度学习的框架中使用了NVIDIA提供的CUDA这个面向GPU计算的综合开发环境。</p><h3 id="分布式学习"><a href="#分布式学习" class="headerlink" title="分布式学习"></a>分布式学习</h3><p>虽然通过GPU可以实现深度学习运算的高速化，但即便如此，当网络较深时，学习还是需要几天到几周的时间。将深度学习的学习过程扩展开来的想法（也就是分布式学习）就变得重要起来。为了进一步提高深度学习所需的计算的速度，可以考虑在多个GPU或者多台机器上进行分布式计算。现在的深度学习框架中，出现了好几个支持多GPU或者多机器的分布式学习的框架。其中，Google的TensorFlow、微软的CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟·高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果</p><p>在使用CNN进行物体检测的方法中，有一个叫作R-CNN的有名的方法。</p><p><img src="C:\Users\sz.L\AppData\Roaming\Typora\typora-user-images\image-20231227223441101.png" alt="image-20231227223441101"></p><h2 id="Deep-Q-Network（强化学习）"><a href="#Deep-Q-Network（强化学习）" class="headerlink" title="Deep Q-Network（强化学习）"></a>Deep Q-Network（强化学习）</h2><p>让计算机也在摸索试验的过程中自主学习，这称为强化学习（reinforcement learning）,通过计算机与环境的交互不断调整参数.在使用了深度学习的强化学习方法中，有一个叫作Deep Q-Network通称<strong>DQN</strong>。该方法基于被称为Q学习的强化学习算法。</p><p>在Q学习中，为了确定最合适的行动，需要确定一个被称为最优行动价值函数的函数。</p><p>不需要提供游戏的状态只需要输入图像就能够让计算机学习。这是强化学习的最大特点</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
